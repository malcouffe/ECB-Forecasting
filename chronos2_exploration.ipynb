{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploration du modèle Chronos-2\n",
        "\n",
        "Ce notebook permet d'explorer la structure du modèle Chronos-2 depuis Hugging Face.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation et imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration SSL appliquée\n"
          ]
        }
      ],
      "source": [
        "# Configuration SSL pour macOS\n",
        "import ssl\n",
        "import certifi\n",
        "\n",
        "# Utiliser les certificats de certifi\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "print(\"✓ Configuration SSL appliquée\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.1\n",
            "CUDA disponible: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from huggingface_hub import list_repo_files, model_info\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. À propos de Chronos-2\n",
        "\n",
        "**Chronos-2** est un modèle de fondation pour la prévision de séries temporelles développé par Amazon. \n",
        "\n",
        "Il supporte :\n",
        "- Prévisions **univariées** (une seule série)\n",
        "- Prévisions **multivariées** (plusieurs séries simultanément)\n",
        "- Prévisions **informées par des covariables** (variables externes)\n",
        "\n",
        "Tout cela dans une architecture unique et unifiée !\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle sélectionné: amazon/chronos-2\n",
            "Chronos-2 est la dernière version du modèle Amazon pour la prévision de séries temporelles\n"
          ]
        }
      ],
      "source": [
        "# Utiliser le modèle Chronos-2 officiel\n",
        "model_name = \"amazon/chronos-2\"\n",
        "\n",
        "print(f\"Modèle sélectionné: {model_name}\")\n",
        "print(\"Chronos-2 est la dernière version du modèle Amazon pour la prévision de séries temporelles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Informations sur le modèle depuis Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nom du modèle: amazon/chronos-2\n",
            "Auteur: amazon\n",
            "Tags: ['chronos-forecasting', 'safetensors', 't5', 'time series', 'forecasting', 'foundation models', 'pretrained models', 'time-series-forecasting', 'dataset:autogluon/chronos_datasets', 'dataset:Salesforce/GiftEvalPretrain', 'arxiv:2403.07815', 'arxiv:2510.15821', 'license:apache-2.0', 'region:us']\n",
            "Pipeline: time-series-forecasting\n",
            "Dernière modification: 2025-11-05 10:32:11+00:00\n"
          ]
        }
      ],
      "source": [
        "# Récupérer les informations du modèle\n",
        "info = model_info(model_name)\n",
        "\n",
        "print(f\"Nom du modèle: {info.modelId}\")\n",
        "print(f\"Auteur: {info.author}\")\n",
        "print(f\"Tags: {info.tags}\")\n",
        "print(f\"Pipeline: {info.pipeline_tag}\")\n",
        "print(f\"Dernière modification: {info.lastModified}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fichiers dans le repository:\n",
            "  - .gitattributes\n",
            "  - README.md\n",
            "  - config.json\n",
            "  - model.safetensors\n"
          ]
        }
      ],
      "source": [
        "# Lister les fichiers du repository\n",
        "files = list_repo_files(model_name)\n",
        "print(\"\\nFichiers dans le repository:\")\n",
        "for file in files:\n",
        "    print(f\"  - {file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Chargement de la configuration du modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12be08b73c174b6da423166a6e82738c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration du modèle:\n",
            "  - Architecture: t5\n",
            "  - Nombre de couches: 12\n",
            "  - Nombre de têtes d'attention: 12\n",
            "  - Dimension du modèle: 768\n",
            "  - Dimension FFN: 3072\n",
            "  - Taille du vocabulaire: 2\n",
            "  - Longueur max: N/A\n"
          ]
        }
      ],
      "source": [
        "# Charger la configuration sans télécharger les poids\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "\n",
        "print(\"Configuration du modèle:\")\n",
        "print(f\"  - Architecture: {config.model_type}\")\n",
        "print(f\"  - Nombre de couches: {config.num_layers}\")\n",
        "print(f\"  - Nombre de têtes d'attention: {config.num_heads}\")\n",
        "print(f\"  - Dimension du modèle: {config.d_model}\")\n",
        "print(f\"  - Dimension FFN: {config.d_ff}\")\n",
        "print(f\"  - Taille du vocabulaire: {config.vocab_size}\")\n",
        "print(f\"  - Longueur max: {config.n_positions if hasattr(config, 'n_positions') else 'N/A'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Configuration complète:\n",
            "T5Config {\n",
            "  \"architectures\": [\n",
            "    \"Chronos2Model\"\n",
            "  ],\n",
            "  \"chronos_config\": {\n",
            "    \"context_length\": 8192,\n",
            "    \"input_patch_size\": 16,\n",
            "    \"input_patch_stride\": 16,\n",
            "    \"max_output_patches\": 64,\n",
            "    \"output_patch_size\": 16,\n",
            "    \"quantiles\": [\n",
            "      0.01,\n",
            "      0.05,\n",
            "      0.1,\n",
            "      0.15,\n",
            "      0.2,\n",
            "      0.25,\n",
            "      0.3,\n",
            "      0.35,\n",
            "      0.4,\n",
            "      0.45,\n",
            "      0.5,\n",
            "      0.55,\n",
            "      0.6,\n",
            "      0.65,\n",
            "      0.7,\n",
            "      0.75,\n",
            "      0.8,\n",
            "      0.85,\n",
            "      0.9,\n",
            "      0.95,\n",
            "      0.99\n",
            "    ],\n",
            "    \"time_encoding_scale\": 8192,\n",
            "    \"use_arcsinh\": true,\n",
            "    \"use_reg_token\": true\n",
            "  },\n",
            "  \"chronos_pipeline_class\": \"Chronos2Pipeline\",\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 0.05,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"reg_token_id\": 1,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 2\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Afficher toute la configuration\n",
        "print(\"\\nConfiguration complète:\")\n",
        "print(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Téléchargement et chargement du modèle\n",
        "\n",
        "**Attention**: Le téléchargement peut prendre du temps selon la taille du modèle et votre connexion internet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Téléchargement du modèle Chronos-2 en cours...\n",
            "Note: Le téléchargement peut prendre plusieurs minutes...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85ad39393db64294ad7f0a22f7235ee3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of T5Model were not initialized from the model checkpoint at amazon/chronos-2 and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle Chronos-2 chargé avec succès!\n"
          ]
        }
      ],
      "source": [
        "# Charger le modèle complet Chronos-2\n",
        "print(\"Téléchargement du modèle Chronos-2 en cours...\")\n",
        "print(\"Note: Le téléchargement peut prendre plusieurs minutes...\")\n",
        "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "print(\"Modèle Chronos-2 chargé avec succès!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exploration de la structure du modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Architecture du modèle:\n",
            "T5Model(\n",
            "  (shared): Embedding(2, 768)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(2, 768)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 12)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-11): 11 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(2, 768)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 12)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-11): 11 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Afficher l'architecture du modèle\n",
        "print(\"Architecture du modèle:\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Nombre total de paramètres: 198,230,784\n",
            "Paramètres entraînables: 198,230,784\n",
            "Taille approximative en mémoire: 756.19 MB (float32)\n"
          ]
        }
      ],
      "source": [
        "# Compter les paramètres\n",
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "total, trainable = count_parameters(model)\n",
        "print(f\"\\nNombre total de paramètres: {total:,}\")\n",
        "print(f\"Paramètres entraînables: {trainable:,}\")\n",
        "print(f\"Taille approximative en mémoire: {total * 4 / (1024**2):.2f} MB (float32)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Modules principaux:\n",
            "  - shared: Embedding\n",
            "  - encoder: T5Stack\n",
            "  - decoder: T5Stack\n"
          ]
        }
      ],
      "source": [
        "# Lister tous les modules du modèle\n",
        "print(\"\\nModules principaux:\")\n",
        "for name, module in model.named_children():\n",
        "    print(f\"  - {name}: {type(module).__name__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Couches du modèle (premier niveau):\n",
            "  - shared.weight: torch.Size([2, 768])\n",
            "  - encoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 12])\n",
            "  - encoder.block.0.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.0.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.0.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.0.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.1.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.1.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.1.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.1.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.2.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.2.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.2.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.2.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.3.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.3.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.3.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.3.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.4.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.4.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.4.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.4.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.5.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.5.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.5.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.5.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.6.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.6.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.6.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.6.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.6.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.6.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.6.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.7.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.7.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.7.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.7.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.7.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.7.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.7.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.8.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.8.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.8.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.8.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.8.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.8.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.8.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.8.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.9.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.9.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.9.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.9.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.9.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.9.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.9.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.9.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.10.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.10.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.10.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.10.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.10.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.10.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.10.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.10.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.11.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - encoder.block.11.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - encoder.block.11.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - encoder.block.11.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - encoder.block.11.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.block.11.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - encoder.block.11.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - encoder.block.11.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - encoder.final_layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 12])\n",
            "  - decoder.block.0.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.0.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.0.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.0.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.0.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.0.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.0.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.0.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.0.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.1.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.1.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.1.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.1.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.1.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.1.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.1.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.1.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.1.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.2.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.2.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.2.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.2.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.2.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.2.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.2.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.2.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.2.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.3.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.3.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.3.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.3.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.3.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.3.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.3.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.3.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.3.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.4.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.4.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.4.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.4.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.4.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.4.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.4.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.4.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.4.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.5.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.5.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.5.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.5.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.5.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.5.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.5.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.5.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.5.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.6.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.6.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.6.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.6.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.6.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.6.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.6.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.6.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.6.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.6.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.6.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.6.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.7.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.7.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.7.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.7.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.7.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.7.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.7.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.7.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.7.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.7.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.7.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.7.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.8.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.8.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.8.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.8.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.8.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.8.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.8.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.8.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.8.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.8.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.8.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.8.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.8.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.9.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.9.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.9.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.9.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.9.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.9.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.9.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.9.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.9.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.9.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.9.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.9.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.9.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.10.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.10.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.10.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.10.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.10.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.10.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.10.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.10.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.10.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.10.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.10.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.10.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.10.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.11.layer.0.SelfAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.11.layer.0.SelfAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.11.layer.0.SelfAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.11.layer.0.SelfAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.11.layer.0.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.11.layer.1.EncDecAttention.q.weight: torch.Size([768, 768])\n",
            "  - decoder.block.11.layer.1.EncDecAttention.k.weight: torch.Size([768, 768])\n",
            "  - decoder.block.11.layer.1.EncDecAttention.v.weight: torch.Size([768, 768])\n",
            "  - decoder.block.11.layer.1.EncDecAttention.o.weight: torch.Size([768, 768])\n",
            "  - decoder.block.11.layer.1.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.block.11.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768])\n",
            "  - decoder.block.11.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072])\n",
            "  - decoder.block.11.layer.2.layer_norm.weight: torch.Size([768])\n",
            "  - decoder.final_layer_norm.weight: torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "# Explorer les couches en détail\n",
        "print(\"\\nCouches du modèle (premier niveau):\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"  - {name}: {param.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Analyse des composants clés\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structure de l'Encoder:\n",
            "T5Stack(\n",
            "  (embed_tokens): Embedding(2, 768)\n",
            "  (block): ModuleList(\n",
            "    (0): T5Block(\n",
            "      (layer): ModuleList(\n",
            "        (0): T5LayerSelfAttention(\n",
            "          (SelfAttention): T5Attention(\n",
            "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (relative_attention_bias): Embedding(32, 12)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): T5LayerFF(\n",
            "          (DenseReluDense): T5DenseActDense(\n",
            "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1-11): 11 x T5Block(\n",
            "      (layer): ModuleList(\n",
            "        (0): T5LayerSelfAttention(\n",
            "          (SelfAttention): T5Attention(\n",
            "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): T5LayerFF(\n",
            "          (DenseReluDense): T5DenseActDense(\n",
            "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_layer_norm): T5LayerNorm()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "\n",
            "Nombre de couches dans l'encoder: 4\n"
          ]
        }
      ],
      "source": [
        "# Analyser l'encoder\n",
        "if hasattr(model, 'encoder'):\n",
        "    print(\"Structure de l'Encoder:\")\n",
        "    print(model.encoder)\n",
        "    print(f\"\\nNombre de couches dans l'encoder: {len(list(model.encoder.children()))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structure du Decoder:\n",
            "T5Stack(\n",
            "  (embed_tokens): Embedding(2, 768)\n",
            "  (block): ModuleList(\n",
            "    (0): T5Block(\n",
            "      (layer): ModuleList(\n",
            "        (0): T5LayerSelfAttention(\n",
            "          (SelfAttention): T5Attention(\n",
            "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (relative_attention_bias): Embedding(32, 12)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): T5LayerCrossAttention(\n",
            "          (EncDecAttention): T5Attention(\n",
            "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): T5LayerFF(\n",
            "          (DenseReluDense): T5DenseActDense(\n",
            "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1-11): 11 x T5Block(\n",
            "      (layer): ModuleList(\n",
            "        (0): T5LayerSelfAttention(\n",
            "          (SelfAttention): T5Attention(\n",
            "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): T5LayerCrossAttention(\n",
            "          (EncDecAttention): T5Attention(\n",
            "            (q): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (k): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (o): Linear(in_features=768, out_features=768, bias=False)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): T5LayerFF(\n",
            "          (DenseReluDense): T5DenseActDense(\n",
            "            (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
            "            (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_layer_norm): T5LayerNorm()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "\n",
            "Nombre de couches dans le decoder: 4\n"
          ]
        }
      ],
      "source": [
        "# Analyser le decoder\n",
        "if hasattr(model, 'decoder'):\n",
        "    print(\"Structure du Decoder:\")\n",
        "    print(model.decoder)\n",
        "    print(f\"\\nNombre de couches dans le decoder: {len(list(model.decoder.children()))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Informations sur les embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Exemple d'utilisation de Chronos-2 pour la prévision\n",
        "\n",
        "Voici comment utiliser Chronos-2 pour faire des prévisions sur vos propres données.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matthieualcouffe/projet/ECB_Forecast/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # requires: pip install 'pandas[pyarrow]'\n",
        "from chronos import Chronos2Pipeline\n",
        "\n",
        "pipeline = Chronos2Pipeline.from_pretrained(\"amazon/chronos-2\", device_map=\"mps\")\n",
        "\n",
        "# Load historical target values and past values of covariates\n",
        "context_df = pd.read_parquet(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/electricity_price/train.parquet\")\n",
        "\n",
        "# (Optional) Load future values of covariates\n",
        "test_df = pd.read_parquet(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/electricity_price/test.parquet\")\n",
        "future_df = test_df.drop(columns=\"target\")\n",
        "\n",
        "# Generate predictions with covariates\n",
        "pred_df = pipeline.predict_df(\n",
        "    context_df,\n",
        "    future_df=future_df,\n",
        "    prediction_length=24,  # Number of steps to forecast\n",
        "    quantile_levels=[0.1, 0.5, 0.9],  # Quantiles for probabilistic forecast\n",
        "    id_column=\"id\",  # Column identifying different time series\n",
        "    timestamp_column=\"timestamp\",  # Column with datetime information\n",
        "    target=\"target\",  # Column(s) with time series values to predict\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
